{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RAG examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from travel_agent.rag.search import RagSearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_client = QdrantClient(url=getenv(\"QDRANT_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Dummy retrieval by full query\n",
    "\n",
    "Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"intfloat/multilingual-e5-large-instruct\"\n",
    "LLM_MODEL = \"Qwen/Qwen1.5-4B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel_agent.qdrant.mosru_places import MosruPlaces\n",
    "\n",
    "# Upload to qdrant\n",
    "MosruPlaces(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "rag = RagSearch(qdrant_client, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(query, results) -> str:\n",
    "    places_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"- {r['Name']} ({r['Type']}) по адресу: {r['Address']}, {r['District']}\"\n",
    "            for r in results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f'Запрос: \"{query}\"\\n'\n",
    "        f\"Топ результатов по запросу:\\n{places_text}\\n\"\n",
    "        \"Суммаризируй информацию в дружелюбной и информативной форме.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Где находится Мастерская Рыбникова?\"\n",
    "\n",
    "retrieved_data = rag.search_places(query)\n",
    "\n",
    "prompt = format_prompt(query, retrieved_data)\n",
    "print(f\"*Prompt:*\\n\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qwen_pipeline = pipeline(\"text-generation\", model=LLM_MODEL, device=DEVICE)\n",
    "response = qwen_pipeline(prompt, max_length=512, return_full_text=False)\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
